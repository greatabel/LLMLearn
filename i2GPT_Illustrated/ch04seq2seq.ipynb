{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c0eb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 导入 numpy\n",
    "import torch # 导入 torch\n",
    "import random # 导入 random 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a79c9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 句子数量： 5\n",
      " 中文词汇表大小： 18\n",
      " 英文词汇表大小： 20\n",
      " 中文词汇到索引的字典： {'喜欢': 0, '自然': 1, '处理': 2, '非常': 3, '深度学习': 4, '改变': 5, '学习': 6, '世界': 7, '咖哥': 8, '爱': 9, '我': 10, '语言': 11, '小冰': 12, '很': 13, '神经网络': 14, '复杂': 15, '强大': 16, '人工智能': 17}\n",
      " 英文词汇到索引的字典： {'studying': 0, 'are': 1, '<eos>': 2, 'Neural-Nets': 3, 'powerful': 4, 'so': 5, 'love': 6, 'is': 7, 'the': 8, 'DL': 9, 'world': 10, 'KaGe': 11, 'AI': 12, 'NLP': 13, 'likes': 14, 'changed': 15, 'I': 16, '<sos>': 17, 'XiaoBing': 18, 'complex': 19}\n"
     ]
    }
   ],
   "source": [
    "# 构建语料库，每行包含中文、英文（解码器输入）和翻译成英文后的目标输出 3 个句子\n",
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰', '<sos> KaGe likes XiaoBing', 'KaGe likes XiaoBing <eos>'],\n",
    "    ['我 爱 学习 人工智能', '<sos> I love studying AI', 'I love studying AI <eos>'],\n",
    "    ['深度学习 改变 世界', '<sos> DL changed the world', 'DL changed the world <eos>'],\n",
    "    ['自然 语言 处理 很 强大', '<sos> NLP is so powerful', 'NLP is so powerful <eos>'],\n",
    "    ['神经网络 非常 复杂', '<sos> Neural-Nets are complex', 'Neural-Nets are complex <eos>']]\n",
    "word_list_cn, word_list_en = [], []  # 初始化中英文词汇表\n",
    "# 遍历每一个句子并将单词添加到词汇表中\n",
    "for s in sentences:\n",
    "    word_list_cn.extend(s[0].split())\n",
    "    word_list_en.extend(s[1].split())\n",
    "    word_list_en.extend(s[2].split())\n",
    "# 去重，得到没有重复单词的词汇表\n",
    "word_list_cn = list(set(word_list_cn))\n",
    "word_list_en = list(set(word_list_en))\n",
    "# 构建单词到索引的映射\n",
    "word2idx_cn = {w: i for i, w in enumerate(word_list_cn)}\n",
    "word2idx_en = {w: i for i, w in enumerate(word_list_en)}\n",
    "# 构建索引到单词的映射\n",
    "idx2word_cn = {i: w for i, w in enumerate(word_list_cn)}\n",
    "idx2word_en = {i: w for i, w in enumerate(word_list_en)}\n",
    "# 计算词汇表的大小\n",
    "voc_size_cn = len(word_list_cn)\n",
    "voc_size_en = len(word_list_en)\n",
    "print(\" 句子数量：\", len(sentences)) # 打印句子数\n",
    "print(\" 中文词汇表大小：\", voc_size_cn) # 打印中文词汇表大小\n",
    "print(\" 英文词汇表大小：\", voc_size_en) # 打印英文词汇表大小\n",
    "print(\" 中文词汇到索引的字典：\", word2idx_cn) # 打印中文词汇到索引的字典\n",
    "print(\" 英文词汇到索引的字典：\", word2idx_en) # 打印英文词汇到索引的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a03b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 原始句子： ['我 爱 学习 人工智能', '<sos> I love studying AI', 'I love studying AI <eos>']\n",
      " 编码器输入张量的形状： torch.Size([1, 4])\n",
      " 解码器输入张量的形状： torch.Size([1, 5])\n",
      " 目标张量的形状： torch.Size([1, 5])\n",
      " 编码器输入张量： tensor([[10,  9,  6, 17]])\n",
      " 解码器输入张量： tensor([[17, 16,  6,  0, 12]])\n",
      " 目标张量： tensor([[16,  6,  0, 12,  2]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义一个函数，随机选择一个句子和词汇表生成输入、输出和目标数据\n",
    "def make_data(sentences):\n",
    "    # 随机选择一个句子进行训练\n",
    "    random_sentence = random.choice(sentences)\n",
    "    # 将输入句子中的单词转换为对应的索引\n",
    "    encoder_input = np.array([[word2idx_cn[n] for n in random_sentence[0].split()]])\n",
    "    # 将输出句子中的单词转换为对应的索引\n",
    "    decoder_input = np.array([[word2idx_en[n] for n in random_sentence[1].split()]])\n",
    "    # 将目标句子中的单词转换为对应的索引\n",
    "    target = np.array([[word2idx_en[n] for n in random_sentence[2].split()]])\n",
    "    # 将输入、输出和目标批次转换为 LongTensor\n",
    "    encoder_input = torch.LongTensor(encoder_input)\n",
    "    decoder_input = torch.LongTensor(decoder_input)\n",
    "    target = torch.LongTensor(target)\n",
    "    return encoder_input, decoder_input, target \n",
    "# 使用 make_data 函数生成输入、输出和目标张量\n",
    "encoder_input, decoder_input, target = make_data(sentences)\n",
    "for s in sentences: # 获取原始句子\n",
    "    if all([word2idx_cn[w] in encoder_input[0] for w in s[0].split()]):\n",
    "        original_sentence = s\n",
    "        break\n",
    "print(\" 原始句子：\", original_sentence) # 打印原始句子\n",
    "print(\" 编码器输入张量的形状：\", encoder_input.shape)  # 打印输入张量形状\n",
    "print(\" 解码器输入张量的形状：\", decoder_input.shape) # 打印输出张量形状\n",
    "print(\" 目标张量的形状：\", target.shape) # 打印目标张量形状\n",
    "print(\" 编码器输入张量：\", encoder_input) # 打印输入张量\n",
    "print(\" 解码器输入张量：\", decoder_input) # 打印输出张量\n",
    "print(\" 目标张量：\", target) # 打印目标张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92d74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d5eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebfecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
