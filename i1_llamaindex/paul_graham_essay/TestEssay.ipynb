{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541e9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT OPENAI KEY\"\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d6d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/blob/main/examples/paul_graham_essay/TestEssay.ipynb\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c51b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index import TreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计数\n",
    "\n",
    "import tiktoken\n",
    "from llama_index.llms import Anthropic\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    set_global_service_context,\n",
    ")\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# 设置tokenizer和TokenCountingHandler\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\").encode\n",
    "\n",
    "token_counter = TokenCountingHandler(tokenizer=tokenizer)\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5010abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43829dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "new_index = TreeIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e592444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  7004 \n",
      " LLM Completion Tokens:  407 \n",
      " Total LLM Token Count:  7411 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e83e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = new_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aab86d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author wrote short stories and also worked on programming, specifically on an IBM 1401 computer in their junior high school's basement. They used an early version of Fortran and typed programs on punch cards. Later, the author got a microcomputer, a TRS-80, and started programming on it, writing simple games and a word processor.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e7ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [10]/[10]\n",
      ">[Level 1] Selected node: [10]/[10]\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6579235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>After his time at Y Combinator, the author worked on building a new dialect of Lisp called Arc.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7ec5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  17632 \n",
      " LLM Completion Tokens:  815 \n",
      " Total LLM Token Count:  18447 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6603e",
   "metadata": {},
   "source": [
    "# Build Tree Index with a custom Summary Prompt, directly retrieve answer from root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a863b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435828be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "query_str = \"What did the author do growing up?\"\n",
    "SUMMARY_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    f\"answer the question: {query_str}\\n\"\n",
    ")\n",
    "SUMMARY_PROMPT = PromptTemplate(SUMMARY_PROMPT_TMPL)\n",
    "index_with_query = TreeIndex.from_documents(documents, summary_template=SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87a4473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.tree_root_retriever:> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n"
     ]
    }
   ],
   "source": [
    "# directly retrieve response from root nodes instead of traversing tree\n",
    "query_engine = index_with_query.as_query_engine(retriever_mode=\"root\")\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a36c4caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author engaged in activities such as writing short stories and programming, including working on an IBM 1401 computer in 9th grade and teaching themselves Lisp. They also worked on reverse-engineering a program called SHRDLU for their undergraduate thesis.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dd2a9",
   "metadata": {},
   "source": [
    "# Using GPT Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45121e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import KeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb05ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = KeywordTableIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ddd6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['author', 'combinator', 'time', 'y combinator']\n",
      "query keywords: ['author', 'combinator', 'time', 'y combinator']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['combinator', 'time', 'y combinator']\n",
      "> Extracted keywords: ['combinator', 'time', 'y combinator']\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeb45779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>After his time at Y Combinator, the author pursued a new project in Cambridge. He formed a team and began working on a web application for building web applications. However, he had a change of heart during the summer and decided not to continue running a company. Instead, he chose to transform a part of the project into an open-source initiative. He then shifted his attention to developing a new Lisp dialect called Arc. Eventually, he presented his work at a Lisp conference and shared a postscript file of the talk online, which generated considerable interest.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db136606",
   "metadata": {},
   "source": [
    "# Using GPT List Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99073013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0d9ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build summary index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1e94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efe10f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The context information does not provide any information about the author's time at Y Combinator or what they did after it.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "412ce639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  74947 \n",
      " LLM Completion Tokens:  4225 \n",
      " Total LLM Token Count:  79172 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874ca5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
