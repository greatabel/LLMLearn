{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "541e9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT OPENAI KEY\"\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31d6d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/blob/main/examples/paul_graham_essay/TestEssay.ipynb\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c51b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index import TreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "354ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计数\n",
    "\n",
    "import tiktoken\n",
    "from llama_index.llms import Anthropic\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    set_global_service_context,\n",
    ")\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# 设置tokenizer和TokenCountingHandler\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\").encode\n",
    "\n",
    "token_counter = TokenCountingHandler(tokenizer=tokenizer)\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5010abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43829dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "new_index = TreeIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e592444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  7004 \n",
      " LLM Completion Tokens:  367 \n",
      " Total LLM Token Count:  7371 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e83e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = new_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3aab86d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author worked on writing and programming outside of school before college. They wrote short stories and tried programming on an IBM 1401 computer using an early version of Fortran. They also mentioned getting a microcomputer, a TRS-80, and started programming more extensively, including writing simple games and a word processor.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4e7ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n",
      ">[Level 1] Selected node: [5]/[5]\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6579235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>After the author's time at Y Combinator, they decided to pursue painting. They wanted to see how good they could get if they focused on it. They spent most of the rest of 2014 painting, but eventually ran out of steam and stopped working on it. They then started writing essays again and later began working on Lisp.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a7ec5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  17246 \n",
      " LLM Completion Tokens:  779 \n",
      " Total LLM Token Count:  18025 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6603e",
   "metadata": {},
   "source": [
    "# Build Tree Index with a custom Summary Prompt, directly retrieve answer from root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a863b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "435828be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "query_str = \"What did the author do growing up?\"\n",
    "SUMMARY_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    f\"answer the question: {query_str}\\n\"\n",
    ")\n",
    "SUMMARY_PROMPT = PromptTemplate(SUMMARY_PROMPT_TMPL)\n",
    "index_with_query = TreeIndex.from_documents(documents, summary_template=SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87a4473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.tree.tree_root_retriever:> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n",
      "> Starting query: What did the author do growing up?\n"
     ]
    }
   ],
   "source": [
    "# directly retrieve response from root nodes instead of traversing tree\n",
    "query_engine = index_with_query.as_query_engine(retriever_mode=\"root\")\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a36c4caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author engaged in activities such as writing short stories and programming, including working on an IBM 1401 computer in 9th grade and teaching themselves Lisp. They also worked on reverse-engineering a program called SHRDLU for their undergraduate thesis.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dd2a9",
   "metadata": {},
   "source": [
    "# Using GPT Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45121e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import KeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb05ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = KeywordTableIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ddd6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "query keywords: ['y combinator', 'author', 'combinator', 'time']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['y combinator', 'combinator', 'time']\n",
      "> Extracted keywords: ['y combinator', 'combinator', 'time']\n",
      "> Extracted keywords: ['y combinator', 'combinator', 'time']\n",
      "> Extracted keywords: ['y combinator', 'combinator', 'time']\n",
      "> Extracted keywords: ['y combinator', 'combinator', 'time']\n",
      "> Extracted keywords: ['y combinator', 'combinator', 'time']\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eeb45779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The context does not provide any information about what the author did after his time at Y Combinator.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db136606",
   "metadata": {},
   "source": [
    "# Using GPT List Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "99073013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0d9ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build summary index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc1e94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "efe10f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The context information does not provide any information about the author's time at Y Combinator or what they did after it.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "412ce639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  73300 \n",
      " LLM Completion Tokens:  3629 \n",
      " Total LLM Token Count:  76929 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874ca5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
